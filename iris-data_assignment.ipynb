{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo9pkE8UFB26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD-_spmPFB3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "06a48a9a-2a0f-4eac-ec98-2b981a9eedce"
      },
      "source": [
        "iris = pd.read_csv('./iris_data.csv')\n",
        "iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnEPX9oXFB3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45b40acb-cc80-4b9f-8605-978c89cc0414"
      },
      "source": [
        "iris.species.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3Xn8oXFB3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris.loc[iris['species']=='virginica','species']=0\n",
        "iris.loc[iris['species']=='versicolor','species']=1\n",
        "iris.loc[iris['species']=='setosa','species'] = 2\n",
        "iris = iris[iris['species']!=2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJYOR_EJFB4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris[['petal_length', 'petal_width']].values.T        \n",
        "Y = iris[['species']].values.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUYqbkn3FB4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd6a6a65-dcd7-4b3b-da59-cafba7eacc1f"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp_7MKPAFB5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15f711b4-9342-49b3-dda7-4410202bfbce"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBTLss-FB5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f6934a1a-2a10-4b69-9984-9e0014235cad"
      },
      "source": [
        "plt.scatter(X[0, :], X[1, :], c=Y[0,:], s=10, cmap=plt.cm.Spectral);\n",
        "plt.title(\"IRIS DATA | Blue - Versicolor, Red - Virginica \")\n",
        "plt.xlabel('Petal Length')\n",
        "plt.ylabel('Petal Width')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzg6Qa9FB5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer_sizes(X, Y):\n",
        "    n_x = X.shape[0] #number of input features\n",
        "    n_h = 1          #number of neurons in the hidden layer\n",
        "    n_y = Y.shape[0] #number of neurons in output layer\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni1GbPZcFB5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):   \n",
        "    np.random.seed(41)\n",
        "    # initialise weights and bias for neuron 1 \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "\n",
        "    #initialise weights and bias for neuron 2\n",
        "    W2 = np.random.randn(n_y, n_h)\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpVdGpcyFB5_",
        "colab_type": "text"
      },
      "source": [
        "### Linear Forward \n",
        "\n",
        "Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION where ACTIVATION will be either tanH or Sigmoid. \n",
        "- [LINEAR -> tanH] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
        "\n",
        "The linear forward module (vectorized over all the examples) computes the following equations:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "where $A^{[0]} = X$. \n",
        "\n",
        "### Linear-Activation Forward\n",
        "\n",
        "In this notebook, you will use two activation functions:\n",
        "\n",
        "- **tanH**: $A = tanH(Z) = tanH(W A + b)$.\n",
        "``` python\n",
        "```\n",
        "\n",
        "- **Sigmoid**: $A = \\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
        "``` python\n",
        "```\n",
        "\n",
        "For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR->ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.\n",
        "\n",
        "**Exercise**: Build the *LINEAR* part of forward propagation.\n",
        "    \n",
        "**Exercise**: Implement the forward propagation of the *LINEAR->ACTIVATION* layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or tanh().\n",
        "\n",
        "**Reminder**:\n",
        "The mathematical representation of the *LINEAR* unit is $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. You may also find `np.dot()` useful. If your dimensions don't match, printing `W.shape` may help. While implementing the activation functions,`np.tanh()` and `np.exp()` may be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJtwa2PFB6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # neuron 1 \n",
        "    Z1 = np.dot(W1, X) + b1       # W1.X + b1\n",
        "    A1 =                          # activation function to be added   \n",
        "\n",
        "    # neuron 2\n",
        "    Z2 = np.dot(W2, A1) + b2      # W2.A1 + b2\n",
        "    A2 =                          # activation function to be added \n",
        "    \n",
        "    return A2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B93mJo0CFB6K",
        "colab_type": "text"
      },
      "source": [
        "Now you need to compute the cost, because you want to check if your model is actually learning.\n",
        "\n",
        "**Exercise**: Compute the cross-entropy cost, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y\\log\\left(A^{[l]}\\right) + (1-y)\\log\\left(1- A^{[l]}\\right))Â \\tag{7}$$\n",
        "\n",
        "**Reminder**:\n",
        "While implementing the cost function,`np.log()`, `np.multiply()` and `np.sum()` may be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADX3QU7rFB6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(A2, Y):\n",
        "   \n",
        "    m = Y.shape[1] # number of training examples\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
        "    loss = - np.sum(logprobs) / m \n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz4ds-ROFB6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NN model ###\n",
        "def nn_model(X, Y):\n",
        "    np.random.seed(41)\n",
        "\n",
        "    #intitlaising number of inputs, hidden neurons and output neurons\n",
        "    n_x = layer_sizes(X, Y)[0]          \n",
        "    n_h = layer_sizes(X, Y)[1]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    parameters = initialize_parameters(n_x, n_h, n_y) #intialise the weights and bias\n",
        "         \n",
        "    # Forward propagation. Inputs: \"X, parameters\". Outputs: \"yhat\".\n",
        "    yhat = forward_propagation(X, parameters)\n",
        "    \n",
        "    # Loss function. Inputs: \"Yhat, Y\". Outputs: \"loss\".\n",
        "    loss = compute_loss(yhat, Y)\n",
        "\n",
        "    print (\"Loss : %.2f\" %(loss*100),\"%\")\n",
        "    print (\"Accuracy : %.2f\" %(100 - (loss*100)),\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1P1HCUHFB6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6b2d559a-a5d1-4cd9-f374-78745d48dea1",
        "tags": []
      },
      "source": [
        "nn_model(X, Y)  #run the neural network"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final"
    },
    "colab": {
      "name": "Iris_data.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}